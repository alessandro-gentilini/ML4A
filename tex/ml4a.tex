\documentclass[12pt, letterpaper]{article}
\usepackage{xcolor}

% typesetting issues
\linespread{1.08333} % 10/13 spacing
\setlength{\topmargin}{-0.25in}
\setlength{\textheight}{9.25in}
\setlength{\headheight}{0.00in}
\setlength{\headsep}{0.00in}
\setlength{\parindent}{1.10\baselineskip}

% text macros
\newcommand{\documentname}{\textsl{Note}}
\newcommand{\todo}[1]{\textcolor{red}{#1}}  % gotta have \usepackage{xcolor} in main doc or this won't work

\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing

\section*{Machine learning for astronomers%
\footnote{This \documentname\ is copyright 2017 the author. Feel free to copy and
distribute it, provided that you make no changes to it whatsoever.}}

\noindent
\textbf{David W. Hogg%
\footnote{For many of the ideas in this \documentname, I am indebted to
  Jo Bovy (Toronto),
  Rob Fergus (NYU),
  Dan Foreman-Mackey (Flatiron),
  Jennifer Hill (NYU),
  Iain Murray (Edinburgh),
  Sam Roweis (deceased),
  and
  Bernhard Sch\"olkopf (MPI-IS).}}\\
\textsl{\footnotesize
  Center for Cosmology and Particle Physics, Department of Physics, New York University \\
  Center for Data Science, New York University \\
  Max-Planck-Institut f\"ur Astronomie, Heidelberg \\
  Flatiron Institute, a division of the Simons Foundation}

\paragraph{Abstract:}
Machine learning---which is hard to define uncontroversially
but which involves expertise-eschewing, data-driven methods for classification,
regression, dimensionality reduction, density estimation, and clustering,
with methods that have great flexibility or even non-parametric form---has transformed all
of the sciences and commerce.
Naturally, it is appearing in many contexts in astronomy and astrophysics and
in such numbers that any kind of review or analysis would now be a fool's errand.
Here I give a very personal discussion of machine-learning methods and lay out the costs and
benefits of their adoption.
And I provide some unsolicited advice!
I highlight the five simplest and most elegant methods I know: support vector
machines, linear regression, principal components analyis, Gaussian mixture models
with expectation maximization, and k-means.
I then discuss their limitations, generalizations, and alternatives, with a focus
on some key qualities:
For most astronomical contexts, we want methods that are interpretable (at least
partially), generalizable (in at least some respects), probabilistic (in that
there is something akin to a likelihood function involved, such that the method can
be inserted into some bigger inference with informed causal structure).
Very few high-performance or computationally tractable methods have any of these
properties, and therefore (in my view),
\emph{most machine-learning methods are not advisable for most astronomical
applications!}
Of course there are exceptions, which I discuss.
Additionally, I spend some time on the magic of kernels and non-parametrics, and in particular
I highlight the Gaussian process, which shows great promise for astronomy.
One of the primary goals of this \documentname\ is to attempt to set down in writing
some of the folk or tacit knowledge that is available in the community, but
(nearly) invisible in the literature.

\clearpage\section{Why machine learning in astronomy?}

Astronomy---thanks to enormous support from public and private funding
agencies---is awash in beautiful and informative data, about planets, stars,
black holes, accretion flows,
the interstellar medium, the Milky Way, the local Universe, and the entire
Hubble Volume, including the surface of last scattering.
For all of these observed phenomena, we have physical models, executed
with large computational simulations, of varying degress of maturity,
that explain the observations.
But of course the data are so rich and so informative and so featured
that no physical model (yet) does justice to all of the details of these
data.
That is, there is far more to see and learn from the data than we can
capture with our (necessarily simplified and limited) physical models.
In some cases, there isn't even a working physical model at all.

It is tempting, therefore, to attempt to transform some of the important
scientific questions in astronomy from questions about how physical models are
constrained by data into a more data-driven form.
That is, when the data are far more featured than any model, it is interesting
to ask whether the data can somehow \emph{become} the model.
And indeed, sometimes questions \emph{can} be so transformed.
For example, when we are looking for tiny, periodic exoplanet transit
signals in the noisy light curve of a variable star, we don't necessarily
need an accurate physical model of the stellar variability:
We can use the empirical properties of the variability learned from
other, similar stars.
When we want to distinguish stars from quasars in multi-band imaging, we
don't necessarily need accurate physical models for stars and quasars,
we can use known stars and quasars to teach us the differences.
In cases like these, the data \emph{is} the model.\footnote{\todo{Something about this phrase.}}

Without digressing into a long discussion of precision and accuracy, I
think it is worth noting here that data-driven methods can achieve
unparalleled accuracy for some tasks:
After all, the observed data provide---or can provide---a very
\emph{accurate} model or picture of astrophysical phenomena \emph{in
  the space of the data}.
That is, if the goal is a model that reproduces the data (as it is in
the planet-finding and classification tasks I just mentioned), a
data-driven model is almost guaranteed to beat any physics-based
model.\footnote{\todo{Argument about modeling the residuals, which is
    more-or-less a proof.}}

And there are other equally important tasks for which data-driven
models can achieve unparalleled \emph{precision}:
If a model is given the flexibility to capture informative features
in a data set that are not captured by other, cruder models, then that
model can use those features for inference.
That is, a sufficiently flexible data-driven model can discover information
sources in the data that more inflexible models might not capture.
This has been true, for example, in our data-driven models of stars\footnote{\todo{cite Ness etc}},
where small issues with atomic, molecular, and plasma physics prevent
physical models of stellar photospheres from achieving the full precision
on (say) element abundances available in the data, but a data-driven
model does better (at least in terms of precision).

Here we are going to look at the space of data-driven models that are
referred to as ``machine learning''.
These are generally and currently the most flexible forms of data-driven
models, and they show great promise for some tasks in astronomy.

If I try to intuit the origins of the name ``machine learning'',
I would guess it relates back to a branch of artificial intelligence,
in which it became interesting to know whether a machine (that,
presumably, does not have the experience or assumptions of a human)
can perform scientific tasks that involve generalizing or learning from
data. \todo{Put a footnote here with a corrective to this.}
Indeed, some of my best friends still use the term
this way, counting any computational inference (any use of a machine
to learn parameters or properties of a data set) as machine learning.
Here I am going to use the term more narrowly, to refer to the computational
inferences that are very scientifically agnostic, that make use of extremely
flexible (highly parameterized or even non-parametric) models, and
that are defined with no reference to the particular data or domain in
which they work.
These include many things you have heard (or maybe even used),
including principal components analysis, convolutional neural
networks, random forests, \todo{[t-SNE here]}, auto-encoders,
generative adversarial networks, support vector machines,
and Gaussian processes,
just to name a few\footnote{This short list (given
  that it includes generative adversarial networks but does not
  include whatever's next) will stand as a very precise time-stamp for
  when this \documentname\ was written.}
that have appeared in the astronomical literature.

Once again: Why in astronomy?
...Problems that only humans can solve at present...Nuisances we don't
care about...Operational situations, where speed is of the essence.

\section{Who are you?}

I am assuming that anyone reading this \documentname\ has a
significant fraction of the following properties:

\section{Basic taxonomy of machine-learning tasks}

...Warning: This \documentname\ will be extremely non-linear, because
the connections across different subjects are myriad. If you know what
you are doing, and all you are looking for is my polemical advice, you
want to skip to \todo{where?}. If you just want to skim the relevant
algorithm names to get some ideas for some application you have in
mind, you can just read (or skim) \todo{what?}.

...In the next sections we are going to give canonical algos (meaning:
algos everyone should know, because they are gorgeous and simple) and
some pointers to more sophisticated stuff. Then we will talk about
statistical, conceptual, and mathematical considerations that cut
across applications and algorithms.

\section{Classification}

...Supervised. In astro: Star--galaxy, high-redshift quasars, cosmic rays.

...SVM

...kSVM. Concept of non-parametrics, to be discussed below.

...Rando Forest

...NNs, CNNs, RNNs, GANs, etc

...What is classification to a bayesian, and how does this relate?

\section{Regression}

...Supervised; relationship to classification. In astro: The Cannon. Kepler and LIGO systematics.

...Linear regression!

...Rando Forest, CNNs, and all the stuff from last section.

...Gaussian Processes (and we will elaborate more later). Likelihood function appears!

\section{Dimensionality reduction}

...Unsupervised. In astro: spectral templates. Feature engineering.

...PCA!

...kPCA. Why don't astronomers use this? The Dual problem problem.

...PPCA, FA, and latent-variable models like GPLVM

...Manifold-learning and t-SNE. Relationships to visualization and feature engineering (more below).

...auto-encoders: Probably useful!

...GANs

\section{Clustering}

...Unsupervised. In astro: SNe types, spectral types.

...k-means!

...[Here I am weakest]

...Using classifiers or regressions to find outliers. (The opposite of clustering.)

\section{Density estimation}

...Unsupervised. In astro: Comparing populations. Pseudo-LFs and pseudo-posteriors. Other things?

...GMM with E-M algo!

...Relationship to clustering and various confusions around that.

...XD, and why we care. And how did this become a classification algorithm? Connections there.

...RNADE

...GANs: In what sense do these estimate density?

\section{Rectangular data}

...format

...but also issues with missing data (to return below)

...kinds of data that can't be made rectangular (trivially). Specifically mention variable-star light curves.

...importance of feature engineering if you must go rectangular.

...importance of distance metrics if you can't!

\section{Distance metrics}

...Most ML methods are nearest-neighbor methods, in some sense!

\section{Statistical principles}

...train, validate, and test.

...concepts of statistical conservatism

...stationarity requirements

\section{Probabilistic methods}

\section{Concept of non-parametrics}

\section{The kernel trick}

\section{Gaussian processes}

\section{Active learning}

\section{Feature engineering}

\section{Interpretability}

\section{Generalizability}

\section{Unsolicited advice}

...only GPs are fully probabilistic and non-parametric

...model nuisances

...model or locate outliers

...operational contexts

...emulators

\end{document}
